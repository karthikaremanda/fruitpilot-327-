# ğŸˆ FRUITPILOT: Autonomous Drone for Fruit Harvesting

**FRUITPILOT** is an innovative AI-powered autonomous drone system designed for fruit harvesting, focused primarily on detecting and navigating towards mangoes on trees using computer vision. The project integrates object detection (YOLO), drone control (DroneKit), and a web interface for real-time monitoring and target selection.

---

## ğŸš€ Features

- ğŸ¯ **Mango Detection with YOLO**: Detects ripe mangoes on trees using YOLOv8.
- ğŸ›°ï¸ **Autonomous Navigation**: Drone automatically navigates to the detected fruits or user-clicked GPS targets.
- ğŸ§­ **Interactive Map Interface**: Click on the map to send coordinates to the drone.
- ğŸ“ **Live GPS Tracking**: Displays the real-time location of the drone on the map.
- ğŸ§  **AI Integration**: Deep learning-based object detection for precise and efficient fruit recognition.
- ğŸ“¡ **Web-based Dashboard**: Built with HTML, CSS, and JS for intuitive user control and monitoring.

---

## ğŸ§© Project Structure

```
fruitpilot-327-
â”œâ”€â”€ backend/                  # Python server and drone control logic
â”‚   â”œâ”€â”€ yolov8_detection.py   # YOLOv8 object detection
â”‚   â”œâ”€â”€ dronekit_control.py   # DroneKit commands and flight logic
â”‚   â””â”€â”€ api/                  # REST API endpoints
â”œâ”€â”€ frontend/                 # Web interface
â”‚   â”œâ”€â”€ index.html            # Main dashboard UI
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ map.js            # Map interaction and drone position updates
â”‚   â”‚   â””â”€â”€ controls.js       # UI controls for takeoff, land, etc.
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ styles.css        # Styling for the UI
â”œâ”€â”€ models/                   # Pre-trained YOLO models
â”‚   â””â”€â”€ mango_detector.pt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ How to Run the Project

### 1. Clone the Repository

```bash
git clone https://github.com/karthikaremanda/fruitpilot-327-.git
cd fruitpilot-327-
```

### 2. Install Python Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 3. Start Backend Server

```bash
python app.py
```

This runs the Python server that handles YOLO detection and drone control.

### 4. Launch the Frontend

Just open `frontend/index.html` in your browser.

---

## ğŸ§ª Tech Stack

- **Frontend**: HTML, CSS, JavaScript, Leaflet.js
- **Backend**: Python (Flask), DroneKit, OpenCV
- **AI/ML**: YOLOv8 for fruit detection
- **Hardware**: PX4 / ArduPilot-compatible drone

---

## ğŸŒ How It Works

1. **Detects Mangoes**: Camera feed is processed using YOLOv8 model.
2. **Gets GPS or Click Coordinates**: User can select a point on the map.
3. **Sends Coordinates to Drone**: Coordinates are passed via backend to DroneKit.
4. **Drone Navigates**: The drone autonomously flies to the given location or the mango.
5. **Live Location Update**: Position is updated live on the map.

---

## ğŸ“¸ Sample Output

> Add sample YOLO detection screenshots or drone-in-flight photos here.

---

## ğŸ¤– Future Scope

- Integration with robotic arm for automated picking
- Multi-fruit detection and classification
- Solar-powered drone enhancements
- Real-time yield estimation using AI

---

## ğŸ¤ Contributing

Pull requests are welcome! Please open an issue first to discuss changes.

---

## ğŸ“ License

MIT License

---

## ğŸ“¬ Contact

Project maintained by [@karthikaremanda](https://github.com/karthikaremanda)
